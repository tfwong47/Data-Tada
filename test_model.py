#!/usr/bin/env python3
import os
from llama_cpp import Llama

def test_model():
    model_path = "models/phi-2-2.7b.gguf"
    
    print(f"ğŸ” Testing model: {model_path}")
    print(f"ğŸ“ File exists: {os.path.exists(model_path)}")
    
    if os.path.exists(model_path):
        print(f"ğŸ“ File size: {os.path.getsize(model_path) / (1024**3):.2f} GB")
        
        try:
            print("ğŸ¤– Loading model...")
            model = Llama(
                model_path=model_path,
                n_ctx=1024,
                n_threads=8,
                n_gpu_layers=0,
                n_batch=512,
                use_mmap=True,
                use_mlock=False
            )
            print("âœ… Model loaded successfully!")
            
            # Test a simple query
            print("ğŸ§ª Testing response...")
            response = model(
                "Hello, how are you?",
                max_tokens=50,
                temperature=0.5
            )
            print(f"ğŸ“ Response: {response}")
            
        except Exception as e:
            print(f"âŒ Error loading model: {e}")
            print(f"ğŸ” Error type: {type(e).__name__}")
    else:
        print("âŒ Model file not found!")

if __name__ == "__main__":
    test_model()
